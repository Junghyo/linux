## vagrant 계정 로그인
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
chmod 644 ~/.ssh/authorized_keys
ssh localhost

sudo yum install java-1.8.0-openjdk-devel.x86_64
## jdk 설치 후에 

vi ~/.bashrc
export PATH
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64/jre
export PATH=$PATH:$JAVA_HOME/bin
source ~/.bashrc

## hadoop 설치
sudo yum install wget ## <-> curl
wget http://ftp.daumkakao.com/apache/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gz
tar xvfz hadoop-2.6.1.tar.gz
ln -s hadoop-2.6.1 hadoop

vi ~/.bashrc

export HADOOP_PREFIX="$HOME/hadoop"
export PATH=$PATH:$HADOOP_PREFIX/bin
export PATH=$PATH:$HADOOP_PREFIX/sbin
export HADOOP_MAPRED_HOME=${HADOOP_PREFIX}
export HADOOP_COMMON_HOME=${HADOOP_PREFIX}
export HADOOP_HDFS_HOME=${HADOOP_PREFIX}
export YARN_HOME=${HADOOP_PREFIX}

source ~/.bashrc

## hadoop setting
## hadoop-env.sh  /home/vagrant/hadoop/etc/hadoop
vi ~/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=$JAVA_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"

# yarn-env.sh
vi ~/hadoop/etc/hadoop/yarn-env.sh
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"
# core-site.xml
vi ~/hadoop/etc/hadoop/core-site.xml

<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://master:9000</value>
        <final>true</final>
    </property>
</configuration>
## hdfs-site.xml
mkdir /home/vagrant/hadoop/dfs
mkdir /home/vagrant/hadoop/dfs/name
mkdir /home/vagrant/hadoop/dfs/data
vi ~/hadoop/etc/hadoop/hdfs-site.xml

<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/vagrant/hadoop/dfs/name</value>
        <final>true</final>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/vagrant/hadoop/dfs/data</value>
    <final>true</final>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>


# mapred-site.xml
cd ~
mkdir mapred
cd mapred
mkdir system
mkdir local
cd ~/hadoop/etc/hadoop
cp mapred-site.xml.template mapred-site.xml
vi $HADOOP_PREFIX/etc/hadoop/mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapred.system.dir</name>
        <value>file:/home/vagrant/mapred/system</value>
        <final>true</final>
    </property>
    <property>
        <name>mapred.local.dir</name>
        <value>file:/home/vagrant/mapred/local</value>
        <final>true</final>
    </property>
</configuration>

#yarn-site.xml
vi $HADOOP_PREFIX/etc/hadoop/yarn-site.xml
<configuration>
    <!-- Site specific YARN configuration properties -->
    <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
    </property>
    <property>
    <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
</configuration>
# 설정 정상 여부 확인
hdfs namenode -format
sudo vi /etc/hosts
# 삭제 하기 127.0.0.1 master
10.1.1.200 master master
10.1.1.201 slave1 slave1
10.1.1.202 slave2 slave2
sudo -i
passwd 설정

systemctl restart network

su vagrant
# 실행

start-dfs.sh # http://10.1.1.200:50070
start-yarn.sh # http://10.1.1.200:8088

stop-dfs.sh # http://10.1.1.200:50070
stop-yarn.sh # http://10.1.1.200:8088

